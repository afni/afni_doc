#!/usr/bin/env python

# [PT: 18 Sep 2024] start


import sys
import os
import glob

from afnipy import afni_util as au
from afnipy import afni_base as ab

# ------------------------------------------------------------------
# special names, strings and objects for this analysis

# prog name and number of inputs required on cmd line
THIS_PROG = 'make_file_of_afni_atlases_dist.py'
NUM_ARGS  = 2

# list of all possible dset extensions to remove
all_dset_ext = ['.nii.gz', '.nii',
                '+tlrc.HEAD', '+orig.HEAD',
                '+tlrc.BRIK', '+orig.BRIK',
                '+tlrc.BRIK.gz', '+orig.BRIK.gz',
                ]

# special names/strings, defining where to get info from
NAME_DIR  = "web_doc_afni_atlases_images_only"
WEB_DIR   = "https://afni.nimh.nih.gov/pub/dist/doc/supplements"
NAME_TGZ  = NAME_DIR + ".tgz"
WEB_ADDR  = WEB_DIR + '/' + NAME_TGZ

# list of labels, to get order right 
data_label_order = [ 'dist_atlas',
                     'dist_other',
                     'old_atlas',
                    ]
# ... and top section text for each: **keep matching with order+number
# of data_label_order**
data_label_blurbs = [
    # dist_atlas
    """This section contains integer-valued datasets. This includes
popular whole-brain atlases, hemisphere-masks, and various subsets of
regions.""",
    # dist_other
    """This section contains a menagerie datasets that are distributed in
afni_atlases, particularly ones that are used as intermediate
reference datasets for certain AFNI programs. For example, the
`afni_refacer_*` datasets are used for defacing and refacing datasets
in AFNI's `@afni_refacer_run`. Not all of these datasets are
integer-valued.""",
    # old_atlas
    """This section contains integer-valued datasets that have
*previously* been distributed in AFNI, but which are no longer
included by default. In many cases, datasets here have been superseded
or improved upon by others that *are* now distributed (see above). In
other cases, they just seemed to be less widely useful. If users have
questions about what might be a better alternative to datasets
relegated here, please ping on the `Message Board
<https://discuss.afni.nimh.nih.gov>`_."""
]

# ------------------------------------------------------------------

TEXT_CHAP = "tempatl"
TEXT_SEC  = "afni_atlases"

text_label = ".. _{}_{}:".format(TEXT_CHAP, TEXT_SEC)

text_title_desc = \
'''

.. comment:
   
   This file was auto-generated by:
     afni_doc/python_help_scripts/{this_prog}
   See afni_doc/do_doc_build_and_copy.tcsh for how it was run

'''.format(this_prog=THIS_PROG)

text_title_desc+= \
'''
***************************************************
**List of atlases (and more) in afni_atlases_dist**
***************************************************

.. contents:: :local:

This page provides a list of the overlays (atlases, masks, and more)
in the "afni_atlases_dist.tgz" package that is distributed with AFNI
code. Axial and sagittal images are shown for each. The listing is in
alphabetical order.

Rather than copying around common atlas, template and other reference
datasets, you can set an AFNI environment variable to specify that
location with AFNI_GLOBAL_SESSION and AFNI_ATLAS_PATH in your
"~/.afnirc" file.  Then, AFNI programs will be able to find those
files easily. For example, they will then automatically be present in
the AFNI GUI overlay and underlay selection lists to load+visualize.

These datasets might change over time.  Indeed, here we also include
some datasets that we no longer distribute by default.  These are
still available, just by separate download.  In some cases, the
"retirement" from distribution may have been because it did not seem
an optimal choice, so you might want to inquire about a more modern
choice.
'''

# ===================================================================

VERSION   = "1.0"
VER_DATE  = "Sep 20, 2024"
AUTHOR    = "PA Taylor (SSCC, NIMH, NIH)"

help_string = '''
--------------------------------------------------------------------
Helpfile for:    ***  {}  ***
Version num:     {}
Version dat:     {}
Written by:      {}

Takes {} arguments: 
   1) output directory to move to for download+file creation
   2) and an output RST file name (*no* path).
--------------------------------------------------------------------

'''.format(THIS_PROG, VERSION, VER_DATE, AUTHOR, NUM_ARGS)

# =================================================================

class ListMatchInfo:
    def __init__(self, label, maindir, blurb='', verb=0):
        """Create object holding list_match_*.txt info, namely about olay-ulay
filename pairs. The label is useful to have separately as an input
(but it was also used to name the list_match*.txt file, anyways).

Parameters
----------
label : str
    label for this object instantiation, likely used to define the fname
maindir : str
    directory containing list match files and data dirs
blurb : str
    optional string that could be added to the top of a section
"""

        self.verb       = verb                # verbosity level
        self.label      = label               # str, label for this object
        self.maindir    = maindir.rstrip('/') # str, dir holding all text/data
        self.blurb      = blurb               # str, desc for subsection

        # created from expected names
        self.data_dir   = ''                  # str, name of dir with images
        self.fname      = ''                  # str, *.txt file to read+parse

        # pieces to store from reading file
        self.title         = ''
        self.all_olay_ulay = []
        
        # stuff within data dir to make note of
        self.all_cbar      = []
        self.all_img       = []
        
        # section text: main output
        self.section_text   = ''

        # ----- do work
        _check1 = self.make_and_verify_names()
        _check2 = self.read_list_match_file()
        _check3 = self.look_for_images()
        _check4 = self.create_section_text()


    # ---------- methods, etc.

    def create_section_text(self):
        """ """

        # section title
        txt = "\n|\n\n"   # spacing from previous
        txt+= "{}\n{}".format(self.title, '='*self.lentitle)

        # optional blurb at top of section
        if self.blurb :
            txt+= '\n\n'
            txt+= self.blurb
            txt+= '\n\n'

        if (self.label).startswith('old'):
            post_note = "\n   * - *NB: this dataset has been retired*"
        else:
            post_note = ""

        for ii in range(self.nolayulay):
            olay = self.all_olay_ulay[ii][0]
            olay_base = self.get_volume_basename(olay)
            ulay = self.all_olay_ulay[ii][1]

            sub_title = "{olay}".format(
                olay=olay
            )
            sub_title_uline = '^'*len(sub_title)


            txt+= '''

.. _{text_chap}_{text_sec}_{text_subsec}:

{sub_title}
{sub_title_uline}

.. list-table:: 
   :widths: 100

   * - Underlay: {ulay} {post_note}
   * - .. image:: {img}
          :width: 100%
          :align: center

'''.format(text_chap=TEXT_CHAP,
           text_sec=TEXT_SEC,
           text_subsec=olay_base,
           sub_title=sub_title,
           sub_title_uline=sub_title_uline,
           ulay=ulay,
           post_note=post_note,
           img=self.all_img[ii])

        self.section_text = txt

        return 0

    def look_for_images(self):
        """After parsing the list match file, look for the appropriate images
        and cbars"""

        for ii in range(self.nolayulay):
            olay = self.all_olay_ulay[ii][0]
            olay_base = self.get_volume_basename(olay)

            # get cbar; annoying because simple globbing *can* find
            # more than one cbar
            sss = self.data_dir + '/' + 'cbar_' + self.label + '_'
            sss+= olay_base + '.jpg'
            ggg = glob.glob(sss)
            if len(ggg) == 1 :
                self.all_cbar.append(ggg[0])
            else:
                ab.EP("Found {} cbar for basename: {}"
                      "".format(len(ggg), olay_base))

            # get image
            sss = self.data_dir + '/' + 'img_' + self.label + '_'
            sss+= olay_base + '.jpg'
            ggg = glob.glob(sss)
            if len(ggg) == 1 :
                self.all_img.append(ggg[0])
            else:
                ab.EP("Found {} axi img for basename: {}"
                      "".format(len(ggg), olay_base))

        # done
        return 0

    def get_volume_basename(self, x):
        """For a volumetric dset x, return the 'basename': what 3dinfo would
        call '-prefix_noext' """

        # go through all possible ext in list above, and *must* find one
        for ext in all_dset_ext :
            if x.endswith(ext):
                return x[:-len(ext)]

        ab.EP("Could not find valid extension for dset: ", x)

    def read_list_match_file(self):
        """we know the structure of the file: first line a comment, next lines
        2 columns: olay and ulay names"""
        
        fff = open(self.fname, 'r')
        X   = fff.readlines()
        fff.close()

        # checks for formatting
        if not(len(X)):
            ab.EP("Problem reading file '{}': result had no length!"
                  "".format(fname))
        if X[0][0] != '#' :
            ab.EP("Problem reading file '{}': X[0][0] != '#'!"
                  "".format(fname))

        # read header/title
        self.title = X[0][1:].strip()

        # read all olay/ulay pairs
        for row in X[1:] :
            lll = row.split()
            if len(lll) != 2 :
                ab.EP("Problem reading rows in '{}'. "
                      "Need exactly 2 elements, not:\n{}".format(fname, lll))
            self.all_olay_ulay.append([lll[0], lll[1]])

        return 0

    def make_and_verify_names(self):
        """make sure appropriate files/dirs exist"""

        self.fname = self.maindir + '/' 
        self.fname+= 'list_match_' + self.label + '.txt'
        if not(os.path.isfile(self.fname)) :
            ab.EP("Could not find required listmatch file: {}"
                  "".format(self.fname))

        self.data_dir = self.maindir + '/' 
        self.data_dir+= 'data_' + self.label
        if not(os.path.isdir(self.data_dir)) :
            ab.EP("Could not find required data dir: {}"
                  "".format(self.data_dir))

        return 0

    @property
    def nolayulay(self):
        """The total number of olay-ulay pairs."""
        return len(self.all_olay_ulay)

    @property
    def lentitle(self):
        """The length of the title."""
        return len(self.title)

# ---------------------------------------------------------------------------

def get_supplements_dir(web_addr, name_tgz, overwrite=True):
    """Download web_addr/name_tgz, and then unpack name_tgz.
    """

    # check about interpreting name
    if name_tgz.endswith('.tgz') :
        dir_supp = name_tgz[:-4]
    else:
        ab.EP("Download package must be *.tgz to be interpretable here, "
              "but this doesn't appear to be: {}".format(name.tgz))

    # check about overwriting or not
    if not(overwrite) :
        if os.path.isfile(name_tgz) :
            ab.EP("Overwrite is not on, and download package '{}' exists "
                  "here already".format(name_tgz))
        if os.path.isdir(dir_supp) :
            ab.EP("Overwrite is not on, and unpacked dir '{}' exists "
                  "here already".format(dir_supp))
    else:
        if os.path.isfile(name_tgz) :
            ab.WP("Removing preexisting download package: {}"
                  "".format(name_tgz))
            cmd = """\\rm {}""".format(name_tgz)
            com = ab.shell_com(cmd, capture=1)
            com.run()
        if os.path.isdir(dir_supp) :
            ab.WP("Removing preexisting unpacked dir: {}"
                  "".format(dir_supp))
            cmd = """\\rm -rf {}""".format(name_tgz)
            com = ab.shell_com(cmd, capture=1)
            com.run()

    # download
    pkg = web_addr + '/' + name_tgz
    cmd = """curl -O {}""".format(pkg)
    com = ab.shell_com(cmd, capture=1)
    com.run()
    if com.status :
        ab.EP("Unsuccessful download of:\n{}".format(pkg))

    # unpack
    cmd = """tar -xf {}""".format(name_tgz)
    com = ab.shell_com(cmd, capture=1)
    com.run()
    if com.status :
        ab.EP("Unsuccessful untar of:\n{}".format(name_tgz))

    return 0


def parse_cmd_arg(aa):
    Narg = len(aa)
    
    if Narg == 0:
        print( help_string )
        sys.exit(0)
    elif Narg < NUM_ARGS:
        sys.exit("** ERROR: too few args!\n"
                 "   Need one input file name (contains all tips),\n"
                 "   and an output fname.")
    elif Narg > NUM_ARGS:
        sys.exit("** ERROR: too many args! See help.")
    else:
        outdir = aa[0]         # fname
        ofile  = aa[1]         # output file name

        print( "++ Output dir     : {}".format( outdir ))
        print( "++ File to create : {}".format( ofile ))

    return outdir, ofile

def move_to_outdir(outdir, ofile):
    """Manage verifying existence of and movement to new dir"""

    # verify existence of outdir
    if not(os.path.isdir(outdir)) :
        ab.EP("Specified output dir '{}' does not appear to exist"
              "".format(outdir))

    # verify no path spec in ofile
    if '/' in ofile :
        ab.EP("Specified output file '{}' has path info, but it should not!"
              "".format(ofile))

    # OK, move to new dir
    os.chdir(outdir)

    return 0

# ====================================================================
# ====================================================================

if __name__=="__main__":

    # --------------------- get input ------------------------

    ab.IP("Command line:\n{}".format(' '.join(sys.argv)))

    # parse inputs
    (outdir, ofile)  =  parse_cmd_arg(sys.argv[1:])

    # check existence of outdir and non-path-ness of ofile; also move
    _check1 = move_to_outdir(outdir, ofile)

    # download+unpack, and will overwrite pre-existing download dir
    _check2 = get_supplements_dir(WEB_DIR, NAME_TGZ, overwrite=True)
    
    # store main dictionary, and also build main output text file
    info_dict = {}
    full_text = text_label
    full_text+= text_title_desc

    # loop over labels to build docs
    for ii in range(len(data_label_order)):
        label = data_label_order[ii]
        blurb = data_label_blurbs[ii]
        info_obj = ListMatchInfo(label, NAME_DIR, blurb)

        full_text+= info_obj.section_text
        info_dict[label] = info_dict

    # write out text file (will overwrite pre-existing)
    fff = open(ofile, 'w')
    fff.write(full_text)
    fff.close()

    ab.IP("Done writing atlases RST:\n{}".format(ofile))


    sys.exit()
